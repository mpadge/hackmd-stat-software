<!-- Edit the .Rmd not the .md file -->

## Dimensionality Reduction, Clustering, and Unsupervised Learning

This document details standards for Dimensionality Reduction, Clustering, and
Unsupervised Learning Software -- referred to from here on for simplicity as
"Unsupervised Learning Software". Software in this category is distinguished
from Regression Software though the latter aiming to construct or analyse one
or more mappings between two defined data sets (for example, a set of
"independent" data, $X$, and a set of "dependent" data, "Y"), whereas
Unsupervised Learning Software aims to construct or analyse one or more
mappings between a defined set of input or independent data, and a second set
of "output" data which are not necessarily known or given prior to the
analysis. A key distinction in Unsupervised Learning Software and Algorithms is
between that for which output data represent (generally numerical)
transformations of the input data set, and that for which output data are
discrete labels applied to the input data. Examples of the former type include
dimensionality reduction and ordination software and algorithms, and examples
of the latter include clustering and discrete partitioning software and
algorithms.

### Input Data Structures and Validation

- **UL1.0** *Unsupervised Learning Software should explicitly document expected
  format (types or classes) for input data, including descriptions of types or
  classes which are not accepted; for example, specification that software
  accepts only numeric inputs in `vector` or `matrix` form, or that all inputs
  must be in `data.frame` form with both column and row names.*
- **UL1.1** *Unsupervised Learning Software should provide distinct
  sub-routines to assert that all input data is of the expected form, and issue
  informative error messages when incompatible data are submitted.*

The following code demonstrates an example of a routine from the base `stats`
package which fails to meet this standard.

```{r hclust-fail, eval = TRUE, error = TRUE}
d <- dist (USArrests) # example from help file for 'hclust' function
hc <- hclust (d) # okay
hc <- hclust (as.matrix (d))
```

The latter fails, yet issues an uninformative error message that clearly
indicates a failure to provide sufficient checks on the class of input data.

- **UL1.2** *Unsupervised learning which uses row or column names to label
  output objects should assert that input data have non-default row or column
  names, and issue an informative message when these are not provided. (Such
  messages need not necessarily be provided by default, but should at least be
  optionally available.)*

The following code provides simple examples of checks whether row and column
names appear to have generic default values.

```{r row-col-names}
x <- data.frame (matrix (1:10, ncol = 2))
x
```

Generic row names are almost always simple integer sequences, which the
following condition confirms.

```{r rowname-check}
identical (rownames (x), as.character (seq (nrow (x))))
```

Generic column names may come in a variety of formats. The following code uses
a `grep` expression to match any number of characters plus an optional leading
zero followed by a generic sequence of column numbers, appropriate for matching
column names produced by generic construction of `data.frame` objects.

```{r colname-check}
all (vapply (seq (ncol (x)), function (i)
             grepl (paste0 ("[[:alpha:]]0?", i), colnames (x) [i]), logical (1)))
```

Messages should be issued in both of these cases. The following code
illustrates that the `hclust` function does not implement any such checks or
assertions, rather it silently returns an object with default labels.

```{r row-col-names2}
u <- USArrests
rownames (u) <- seq (nrow (u))
hc <- hclust (dist (u))
head (hc$labels)
```

- **UL1.3** *Unsupervised Learning Software should transfer all relevant aspects
  of input data, notably including row and column names, and potentially
  information from other `attributes()`, to corresponding aspects of return
  objects.*
    - **UL1.3a** *Where otherwise relevant information is not transferred,
      this should be explicitly documented.*

An example of a function according with UL1.3 is
[`stats::cutree()`](https://stat.ethz.ch/R-manual/R-patched/library/stats/html/cutree.html)

```{r}
hc <- hclust (dist (USArrests))
head (cutree (hc, 10))
```
The row names of `USArrests` are transferred to the output object. In contrast,
some routines from the [`cluster`
package](https://cran.r-project.org/package=cluster) do not comply with this standard:

```{r}
library (cluster)
ac <- agnes (USArrests) # agglomerative nesting
head (cutree (ac, 10))
```
The case labels are not appropriately carried through to the object returned by
[`agnes()`](https://stat.ethz.ch/R-manual/R-devel/library/cluster/html/agnes.html)
to enable them to be transferred within 
[`cutree()`](https://stat.ethz.ch/R-manual/R-patched/library/stats/html/cutree.html).
(The labels are transferred to the object returned by `agnes`, just not in
a way that enables `cutree` to inherit them.)

- **UL1.4** *Unsupervised Learning Software should explicitly document whether
  input data may include missing values.*
- **UL1.5** *Functions in Unsupervised Learning Software which do not admit
  input data with missing values should provide informative error messages when
  data with missing values are submitted.*
- **UL1.6** *Unsupervised Learning Software should document any assumptions made
  with regard to input data; for example assumptions about distributional forms
  or locations (such as that data are centred or on approximately equivalent
  distributional scales). Implications of violations of these assumptions
  should be both documented and tested, in particular:*
    - **UL1.6a** *Software which responds qualitatively differently to input
      data which has components on markedly different scales should explicitly
      document such differences, and implications of submitting such data.*
    - **UL1.6b** *Examples or other documentation should not use `scale()` or
      equivalent transformations without explaining why scale is applied,
      and explicitly illustrating and contrasting the consequences of not
      applying such transformations.*

### Pre-processing and Variable Transformation

- **UL2.0** *Routines likely to give unreliable or irreproducible results in
  response to violations of assumptions regarding input data (see UL1.6) should
  implement pre-processing steps to diagnose potential violations, and issue
  appropriately informative messages, and/or include parameters to enable
  suitable transformations to be applied (such as the `center` and `scale.`
  parameters of the
  [`stats::prcomp()`](https://stat.ethz.ch/R-manual/R-patched/library/stats/html/prcomp.html)
  function).*
- **UL2.1** *Unsupervised Learning Software should document any transformations
  applied to input data, for example conversion of label-values to `factor`,
  and should provide ways to explicitly avoid any default transformations (with
  error or warning conditions where appropriate).*
- **UL2.2** *For Unsupervised Learning Software which accepts missing values in
  input data, functions should implement explicit parameters controlling the
  processing of missing values, ideally distinguishing `NA` or `NaN` values
  from `Inf` values (for example, through use of `na.omit()` and related
  functions from the `stats` package).*
- **UL2.3** *Unsupervised Learning Software should implement pre-processing
  routines to identify whether aspects of input data are perfectly collinear.*

### Algorithms

#### Labelling

- **UL3.1** *Algorithms which apply sequential labels to input data (such as
  clustering or partitioning algorithms) should ensure that the sequence
  follows decreasing group sizes (so labels of "1", "a", or "A" describe the
  largest group, "2", "b", or "B" the second largest, and so on.)*

Note that the [`stats::cutree()`
function](https://stat.ethz.ch/R-manual/R-patched/library/stats/html/cutree.html)
does not accord with this standard:

```{r cutree}
hc <- hclust (dist (USArrests))
table (cutree (hc, k = 10))
```

The [`cutree()`
function](https://stat.ethz.ch/R-manual/R-patched/library/stats/html/cutree.html)
applies arbitrary integer labels to the groups, yet the order of labels is not
related to the order of group sizes.

- **UL3.2** *Dimensionality reduction or equivalent algorithms which label
  dimensions should ensure that that sequences of labels follows decreasing
  "importance" (for example, eigenvalues or variance contributions).*

The
[`stats::prcomp`](https://stat.ethz.ch/R-manual/R-patched/library/stats/html/prcomp.html)
function accords with this standard:

```{r prcomp}
z <- prcomp (eurodist, rank = 5) # return maximum of 5 components
summary (z)
```

The proportion of variance explained by each component decreasing with
increasing numeric labelling of the components.

#### Prediction

- **UL3.3** *Where applicable, Unsupervised Learning Software should implement
  routines to predict the properties (such as numerical ordinates, or cluster
  memberships) of additional new data without re-running the entire algorithm.*

While many algorithms such as Hierarchical clustering can not (readily) be used
to predict memberships of new data, other algorithms can nevertheless be
applied to perform this task. The following demonstrates how the output of
[`stats::hclust`](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/hclust.html)
can be used to predict membership of new data using the [`class:knn()`
function](https://stat.ethz.ch/R-manual/R-devel/library/class/html/knn.html).
(This is intended to illustrate only one of many possible approaches.)

```{r knn}
library (class)
hc <- hclust (dist (iris [, -5]))
groups <- cutree (hc, k = 3)
# function to randomly select part of a data.frame and # add some randomness
sample_df <- function (x, n = 5) {
    x [sample (nrow (x), size = n), ] + runif (ncol (x) * n)
}
iris_new <- sample_df (iris [, -5], n = 5)
# use knn to predict membership of those new points:
knnClust <- knn (train = iris [, -5], test = iris_new , k = 1, cl = groups)
knnClust
```

The [`stats::prcomp()`
function](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/prcomp.html)
implements its own `predict()` method which conforms to this standard:

```{r predict-prcomp}
res <- prcomp (USArrests)
arrests_new <- sample_df (USArrests, n = 5)
predict (res, newdata = arrests_new)
```

#### Group Distributions and Associated Statistics

Many unsupervised learning algorithms serve to label, categorise, or partition
data. Software which performs any of these tasks will commonly output some kind
of labelling or grouping schemes. The above example of principal components
illustrates that the return object records the standard deviations associated
with each component:

```{r prcomp-return}
res <- prcomp (USArrests)
print(res)
summary (res)
```

Such output accords with the following standard:

- **UL3.4** *Objects returned from Unsupervised Learning Software which labels,
  categorise, or partitions data into discrete groups should include, or
  provide immediate access to, quantitative information on intra-group
  variances or equivalent, as well as on inter-group relationships where
  applicable.*

The above example of principal components is one where there are no inter-group
relationships, and so that standard is fulfilled by providing information on
intra-group variances alone. Discrete clustering algorithms, in contrast, yield
results for which inter-group relationships are meaningful, and such
relationships can generally be meaningfully provided. The [`hclust()`
routine](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/hclust.html),
like many clustering routines, simply returns a *scheme* for devising an
arbitrary number of clusters, and so
can not meaningfully provide variances or relationships between such. The
[`cutree()`
function](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/cutree.html),
however, does yield defined numbers of clusters, yet devoid of any quantitative
information on variances or equivalent.

```{r}
res <- hclust (dist (USArrests))
str (cutree (res, k = 5))
```

Compare that with the output of a largely equivalent routine, the [`clara()`
function](https://stat.ethz.ch/R-manual/R-devel/library/cluster/html/clara.html)
from the [`cluster` package](https://cran.r-project.org/package=cluster).

```{r}
library (cluster)
cl <- clara (USArrests, k = 10) # direct clustering into specified number of clusters
cl$clusinfo
```

That object contains information on dissimilarities between each observation
and cluster medoids, which in the context of UL3.4 is "information on
intra-group variances or equivalent". Moreover, inter-group information is also
available as the
["silhouette"](https://stat.ethz.ch/R-manual/R-devel/library/cluster/html/silhouette.html)
of the clustering scheme.

### Return Results

- **UL4.0** *Unsupervised Learning Software should return some form of "model"
  object, generally through using or modifying existing class structures for
  model objects, or creating a new class of model objects.*
- **UL4.1** *Unsupervised Learning Software may enable an ability to generate
  a model object without actually fitting values. This may be useful for
  controlling batch processing of computationally intensive fitting algorithms.*
- **UL4.2** *The return object from Unsupervised Learning Software should
  include, or otherwise enable immediate extraction of, all parameters used to
  control the algorithm used.*

#### Reporting Return Results

- **UL4.2** *Model objects returned by Unsupervised Learning Software should
  implement or appropriately extend a default `print` method which provides an
  on-screen summary of model (input) parameters and methods used to generate
  results. The `print` method may also summarise statistical aspects of the
  output data or results.*
    - **UL4.2a** *The default `print` method should always ensure only
      a restricted number of rows of any result matrices or equivalent are
      printed to the screen.*

The [`prcomp`
objects](https://stat.ethz.ch/R-manual/R-patched/library/stats/html/prcomp.html)
returned from the function of the same name include potential large matrices of
component coordinates which are by default printed in their entirety to the
screen. This is because the default print behaviour for most tabular objects in
R (`matrix`, `data.frame`, and objects from the `Matrix` package, for example)
is to print objects in their entirety (limited only by such options as
`getOption("max.print")`, which determines maximal numbers of printed objects,
such as lines of `data.frame` objects). Such default behaviour ought be
avoided, particularly in Unsupervised Learning Software which commonly returns
objects containing large numbers of numeric entries.

- **UL4.3** *Unsupervised Learning Software should also implement `summary`
  methods for model objects which should summarise the primary statistics used
  in generating the model (such as numbers of observations, parameters of
  methods applied). The `summary` method may also provide summary statistics
  from the resultant model.*

### Documentation

### Visualization

- **UL6.0** *Objects returned by Unsupervised Learning Software should have
  default `plot` methods, either through explicit implementation, extension of
  methods for existing model objects, through ensuring default methods work
  appropriately, or through explicit reference to helper packages such as
  [`factoextra`](https://github.com/kassambara/factoextra) and associated
  functions.*
- **UL6.1** *Where the default `plot` method is **NOT** a generic `plot` method
  dispatched on the class of return objects (that is, through
  a `plot.<myclass>` function), that method dispatch should nevertheless exist
  in order to explicitly direct users to the appropriate function.*
- **UL6.2** *Where default plot methods include labelling components of return
  objects (such as cluster labels), routines should ensure that labels are
  automatically placed to ensure readability, and/or that appropriate
  diagnostic messages are issued where readability is likely to be compromised
  (for example, through attempting to place too many labels).*

### Testing

